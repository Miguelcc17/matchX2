{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "import apps.matcher as matcher  \n",
    "# import spacy\n",
    "import matcher2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Cargar tokenizador y modelo de HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state  # El último estado oculto contiene todas las incrustaciones de tokens\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def obtener_incrustaciones(texto1, texto2):\n",
    "    # Tokenizar las frases\n",
    "    encoded_input = tokenizer([texto1, texto2], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Calcular incrustaciones de tokens\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Realizar Pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalizar las incrustaciones\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Calcular la similitud del coseno entre las incrustaciones\n",
    "    cosine_similarities = cosine_similarity(sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "    # Escalar los valores de similitud del coseno al rango de 1 a 100\n",
    "    scaled_similarity = cosine_similarities * 100\n",
    "\n",
    "    # Redondear los valores para obtener un resultado del 1 al 100\n",
    "    result = scaled_similarity.round()\n",
    "\n",
    "\n",
    "    return result[0,1]\n",
    "\n",
    "def fuzzy_similarity_filter(df, column1, column2, threshold):\n",
    "    \"\"\"\n",
    "    Filtra un DataFrame basado en la similitud de dos columnas utilizando fuzzy matching.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): El DataFrame que se va a filtrar.\n",
    "        column1 (str): Nombre de la primera columna.\n",
    "        column2 (str): Nombre de la segunda columna.\n",
    "        threshold (int): Umbral de similitud.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: El DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    # with model ia\n",
    "    filtered_df = df[df.apply(lambda x: obtener_incrustaciones(str(x[column1]), str(x[column2])) >= threshold, axis=1)]\n",
    "    # with fuzz\n",
    "    # filtered_df = df[df.apply(lambda x: fuzz.ratio(str(x[column1]), str(x[column2])) >= threshold, axis=1)]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8f in position 6329: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Filtrar el DataFrame utilizando la función fuzzy_similarity_filter\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m casa \u001b[39m=\u001b[39m matcher2\u001b[39m.\u001b[39;49mget_df(\u001b[39m'\u001b[39;49m\u001b[39mfiles/match/tokopedia.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m casa \u001b[39m=\u001b[39m casa[casa[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misna()]\n\u001b[0;32m      4\u001b[0m filtered_df \u001b[39m=\u001b[39m fuzzy_similarity_filter(casa, \u001b[39m'\u001b[39m\u001b[39mproduct_name_lazada\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproduct_name_competitor\u001b[39m\u001b[39m'\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m75\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hi4i\\Desktop\\projects\\new match\\matcher2.py:48\u001b[0m, in \u001b[0;36mget_df\u001b[1;34m(archivo)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m delimitador \u001b[39min\u001b[39;00m posibles_delimitadores:\n\u001b[0;32m     46\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         \u001b[39m# Leer el CSV y convertir a DataFrame\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m         df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(archivo, encoding\u001b[39m=\u001b[39;49mencoding, delimiter\u001b[39m=\u001b[39;49mdelimitador)\n\u001b[0;32m     49\u001b[0m         \u001b[39mbreak\u001b[39;00m  \u001b[39m# Si se lee correctamente, salir del bucle\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[39mexcept\u001b[39;00m pd\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mParserError:\n",
      "File \u001b[1;32mc:\\Users\\hi4i\\Desktop\\projects\\new match\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\hi4i\\Desktop\\projects\\new match\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\hi4i\\Desktop\\projects\\new match\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\hi4i\\Desktop\\projects\\new match\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hi4i\\Desktop\\projects\\new match\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype_backend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[39m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8f in position 6329: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Filtrar el DataFrame utilizando la función fuzzy_similarity_filter\n",
    "casa = matcher2.get_df('files/match/tokopedia.csv')\n",
    "casa = casa[casa['error'].isna()]\n",
    "filtered_df = fuzzy_similarity_filter(casa, 'product_name_lazada', 'product_name_competitor', threshold=75)\n",
    "\n",
    "# with model ia\n",
    "filtered_df.to_csv('result_withmodel_tokopedia.csv')\n",
    "# with fuzz\n",
    "# filtered_df.to_csv('result_j_lvzoveqs2rdcilf301.csv')\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "casa[casa['similarity']>=80].to_csv('mayor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0], [2, 1], [2, 2], [2, 5], [2, 6], [2, 7], [2, 9], [2, 10], [2, 11], [2, 12], [2, 13], [13, 0], [13, 1], [13, 2], [13, 5], [13, 6], [13, 7], [13, 9], [13, 10], [13, 11], [13, 12], [13, 13], [25, 0], [25, 1], [25, 2], [25, 5], [25, 6], [25, 7], [25, 9], [25, 10], [25, 11], [25, 12], [25, 13], [27, 0], [27, 1], [27, 2], [27, 5], [27, 6], [27, 7], [27, 9], [27, 10], [27, 11], [27, 12], [27, 13], [38, 0], [38, 1], [38, 2], [38, 5], [38, 6], [38, 7], [38, 9], [38, 10], [38, 11], [38, 12], [38, 13], [44, 0], [44, 1], [44, 2], [44, 5], [44, 6], [44, 7], [44, 9], [44, 10], [44, 11], [44, 12], [44, 13], [52, 0], [52, 1], [52, 2], [52, 5], [52, 6], [52, 7], [52, 9], [52, 10], [52, 11], [52, 12], [52, 13], [61, 0], [61, 1], [61, 2], [61, 5], [61, 6], [61, 7], [61, 9], [61, 10], [61, 11], [61, 12], [61, 13], [78, 0], [78, 1], [78, 2], [78, 5], [78, 6], [78, 7], [78, 9], [78, 10], [78, 11], [78, 12], [78, 13], [88, 0], [88, 1], [88, 2], [88, 5], [88, 6], [88, 7], [88, 9], [88, 10], [88, 11], [88, 12], [88, 13], [91, 0], [91, 1], [91, 2], [91, 5], [91, 6], [91, 7], [91, 9], [91, 10], [91, 11], [91, 12], [91, 13], [92, 0], [92, 1], [92, 2], [92, 5], [92, 6], [92, 7], [92, 9], [92, 10], [92, 11], [92, 12], [92, 13], [138, 0], [138, 1], [138, 2], [138, 5], [138, 6], [138, 7], [138, 9], [138, 10], [138, 11], [138, 12], [138, 13], [146, 0], [146, 1], [146, 2], [146, 5], [146, 6], [146, 7], [146, 9], [146, 10], [146, 11], [146, 12], [146, 13], [152, 0], [152, 1], [152, 2], [152, 5], [152, 6], [152, 7], [152, 9], [152, 10], [152, 11], [152, 12], [152, 13]]\n"
     ]
    }
   ],
   "source": [
    "matcher2.harmony_datum_matcher('jecsi', ['product_name_en','product_name_competitor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "casa[casa['error'].isna()].to_json('jecsi/alternative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'origin': {'product_name': 'Official Xiaomi Google TV A 32', 'brand_name': 'Xiaomi', 'color': 'Black', 'cat_level_1': 'Televisions %26 Videos', 'cat_level_2': 'Smart Televisions'}, 'alternative': {'product_name_competitor': \"FOR SALE OFFICIAL XIAOMI GOOGLE TV A 32' CHEAPEST - Default\", 'brand': 'Xiaomi', 'category_path': 'electronic/tv-accessories/television', 'color': ''}}\n",
      "Origin Data:\n",
      "{'product_name': 'Official Xiaomi Google TV A 32', 'brand_name': 'Xiaomi', 'color': 'Black', 'cat_level_1': 'Televisions %26 Videos', 'cat_level_2': 'Smart Televisions'}\n",
      "\n",
      "Alternative Data:\n",
      "{'product_name_competitor': \"FOR SALE OFFICIAL XIAOMI GOOGLE TV A 32' CHEAPEST - Default\", 'brand': 'Xiaomi', 'category_path': 'electronic/tv-accessories/television', 'color': ''}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_and_split_data_from_api(api_url, params):\n",
    "    \"\"\"\n",
    "    Sends a GET request to the specified API URL with the given parameters,\n",
    "    retrieves the JSON response, and splits it into two separate dictionaries.\n",
    "\n",
    "    Args:\n",
    "        api_url (str): The URL of the API to send the request to.\n",
    "        params (dict): A dictionary containing the parameters to be sent with the request.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries, the first dictionary \n",
    "        contains the data under the \"origin\" key, and the second dictionary \n",
    "        contains the data under the \"alternative\" key.\n",
    "    \"\"\"\n",
    "    # Send GET request\n",
    "    response = requests.get(api_url, params=params)\n",
    "\n",
    "    # Get JSON response\n",
    "    r = response.json()\n",
    "\n",
    "    # Decode the JSON string\n",
    "    data = json.loads(r[\"responseData\"][\"translatedText\"])\n",
    "    print(data)\n",
    "    # Extract data into separate variables\n",
    "    origin_data = data[\"origin\"]\n",
    "    alternative_data = data[\"alternative\"]\n",
    "\n",
    "    return origin_data, alternative_data\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://api.mymemory.translated.net/get\"\n",
    "\n",
    "# Parameters of the request\n",
    "params = {\n",
    "    \"q\": '{ \"origin\": { \"product_name\": \"Official Xiaomi Google TV A 32\", \"brand_name\": \"Xiaomi\", \"color\": \"Black\", \"cat_level_1\": \"Televisions %26 Videos\", \"cat_level_2\": \"Smart Televisions\" }, \"alternative\": { \"product_name_competitor\": \"FOR SALE OFFICIAL XIAOMI GOOGLE TV A 32\\' TERMURAH - Default\", \"brand\": \"Xiaomi\", \"category_path\": \"electronic/tv-accessories/television\", \"color\": \"\" } }',\n",
    "    \"langpair\": \"id|en\"\n",
    "}\n",
    "\n",
    "# Retrieve and split data from API\n",
    "origin, alternative = get_and_split_data_from_api(url, params)\n",
    "print(\"Origin Data:\")\n",
    "print(origin)\n",
    "print(\"\\nAlternative Data:\")\n",
    "print(alternative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de origen:\n",
      "{'product_name': 'Official Xiaomi Google TV A 32', 'brand_name': 'Xiaomi', 'color': 'Black', 'cat_level_1': 'Televisions %26 Videos', 'cat_level_2': 'Smart Televisions'}\n",
      "\n",
      "Datos alternativos:\n",
      "{'product_name_competitor': \"FOR SALE OFFICIAL XIAOMI GOOGLE TV A 32' CHEAPEST - Default\", 'brand': 'Xiaomi', 'category_path': 'electronic/tv-accessories/television', 'color': ''}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "# URL de la API\n",
    "\n",
    "def \n",
    "url = \"https://api.mymemory.translated.net/get\"\n",
    "\n",
    "# Parámetros de la solicitud\n",
    "params = {\n",
    "    \"q\": '{ \"origin\": { \"product_name\": \"Official Xiaomi Google TV A 32\", \"brand_name\": \"Xiaomi\", \"color\": \"Black\", \"cat_level_1\": \"Televisions %26 Videos\", \"cat_level_2\": \"Smart Televisions\" }, \"alternative\": { \"product_name_competitor\": \"FOR SALE OFFICIAL XIAOMI GOOGLE TV A 32\\' TERMURAH - Default\", \"brand\": \"Xiaomi\", \"category_path\": \"electronic/tv-accessories/television\", \"color\": \"\" } }',\n",
    "    \"langpair\": \"id|en\"\n",
    "}\n",
    "\n",
    "# Realizar la solicitud GET\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "r = response.json()\n",
    "\n",
    "# Decodificar la cadena JSON\n",
    "data = json.loads(r[\"responseData\"][\"translatedText\"])\n",
    "\n",
    "# Separar en dos variables\n",
    "origin_data = data[\"origin\"]\n",
    "alternative_data = data[\"alternative\"]\n",
    "\n",
    "# Imprimir las variables\n",
    "print(\"Datos de origen:\")\n",
    "print(origin_data)\n",
    "print(\"\\nDatos alternativos:\")\n",
    "print(alternative_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"origin\": { \"product_name\": \"Official Xiaomi Google TV A 32\", \"brand_name\": \"Xiaomi\", \"color\": \"Black\", \"cat_level_1\": \"Televisions %26 Videos\", \"cat_level_2\": \"Smart Televisions\"}, \"alternative\": {\"product_name_competitor\": \"FOR SALE OFFICIAL XIAOMI GOOGLE TV A 32\\' CHEAPEST - Default\", \"brand\": \"Xiaomi\", \"category_path\": \"electronic/tv-accessories/television\", \"color\": \"\" } }'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[\"responseData\"][\"translatedText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Función de Pooling Promedio - Toma en cuenta la máscara de atención para un promedio correcto\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  # El último estado oculto contiene todas las incrustaciones de tokens\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Frases para las cuales queremos obtener incrustaciones\n",
    "# sentences = ['hola', 'hello', 'hola']\n",
    "sentences = [\"Face Care Acne Repair Spot Serum: Reduces acne marks in 3 days 15ml\",\n",
    " 'Buy 1 Take 1 NIVEA Face Care Acne Repair Spot Serum to reduce acne marks 15ml']\n",
    "\n",
    "# Cargar tokenizador y modelo de HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenizar las frases\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Calcular incrustaciones de tokens\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Realizar Pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalizar las incrustaciones\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# print(\"Sentence embeddings:\")\n",
    "# print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado del 1 al 100:\n",
      "[[100.  78.]\n",
      " [ 78. 100.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calcular la similitud del coseno entre las incrustaciones\n",
    "cosine_similarities = cosine_similarity(sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "# Escalar los valores de similitud del coseno al rango de 1 a 100\n",
    "scaled_similarity = cosine_similarities * 100\n",
    "\n",
    "# Redondear los valores para obtener un resultado del 1 al 100\n",
    "result = scaled_similarity.round()\n",
    "\n",
    "print(\"Resultado del 1 al 100:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.0\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  # El último estado oculto contiene todas las incrustaciones de tokens\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def obtener_incrustaciones(texto1, texto2):\n",
    "    # Cargar tokenizador y modelo de HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    # Tokenizar las frases\n",
    "    encoded_input = tokenizer([texto1, texto2], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Calcular incrustaciones de tokens\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Realizar Pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalizar las incrustaciones\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Calcular la similitud del coseno entre las incrustaciones\n",
    "    cosine_similarities = cosine_similarity(sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "    # Escalar los valores de similitud del coseno al rango de 1 a 100\n",
    "    scaled_similarity = cosine_similarities * 100\n",
    "\n",
    "    # Redondear los valores para obtener un resultado del 1 al 100\n",
    "    result = scaled_similarity.round()\n",
    "\n",
    "    return result[0,1]\n",
    "\n",
    "# Ejemplo de uso:\n",
    "texto1 = \"SACE LADY Oil Control Powder Waterproof Long-lasting Makeup Setting Powder Easy To Carry Compact Powder And Light Face Makeup Powder With Mirror+Puff\"\n",
    "texto2 = \"Loose Powder with Puff - Light\"\n",
    "\n",
    "incrustaciones = obtener_incrustaciones(texto1, texto2)\n",
    "print(incrustaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52.1 >= 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000004, 0.7517575],\n",
       "       [0.7517575, 1.       ]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "precision = 80\n",
    "series_origin, series_origin_x=matcher2.get_df('prueba2/origin.json')['title'].astype('str'), matcher2.get_df('prueba2/origin.json')['link'].astype('str')\n",
    "series_alternative, series_alternative_x=matcher2.get_df('prueba2/alternative_2.json')['product_name'].astype('str'), matcher2.get_df('prueba2/alternative_2.json')['url'].astype('str')\n",
    "nr = []\n",
    "for index_row_origin, index_row_origin_x in zip(series_origin, series_origin_x):\n",
    "    for index_row_alternative, index_row_alternative_x in zip(series_alternative, series_alternative_x):\n",
    "        if index_row_alternative == 'nan':\n",
    "            continue\n",
    "        radio = fuzz.ratio(index_row_origin, index_row_alternative )\n",
    "        if radio in range(50,79) :\n",
    "            nr.append({'similitud':radio, 'nombre_origin':index_row_origin.lower(),'link_origin': index_row_origin_x.lower(),'nombre_alternative':index_row_alternative.lower(), 'link_alternative': index_row_alternative_x.lower()    })\n",
    "            # print(f'{radio} #-# {index_row_origin} #-# {index_row_alternative}')\n",
    "pd.DataFrame(nr).to_csv('shopee_lazada_range.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher2.get_df('shopee_lazada_range.csv').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos del archivo CSV\n",
    "df = matcher.get_df('colores.csv')\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "# nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = \" \".join([token.lemma_ for token in df])\n",
    "doc = nlp(\"naranja\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word\"] = df[\"word\"].map(lambda x: x.lower())\n",
    "df[\"lemmatized_text\"] = df[\"word\"].apply(lemmatize_text)\n",
    "df[\"tag\"] = \"color\"\n",
    "df[\"language\"] = \"fr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_by_column = df.apply(lambda col: col.astype(str).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "# Establecer una conexión con la base de datos SQLite (si no existe, la creará)\n",
    "engine = create_engine('sqlite:///data.sqlite', echo=True)\n",
    "\n",
    "# Guardar el DataFrame en la base de datos (reemplazando si ya existe)\n",
    "df.to_sql('LemmatizedKeywords', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar la tabla para verificar si se ha cargado correctamente\n",
    "consulta = pd.read_sql_query('SELECT * FROM LemmatizedKeywords', engine)\n",
    "print(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta.to_csv(\"respaldo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12283303]]\n",
      "Products are not a match\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example product data from two links\n",
    "product_a = {    \"description\": \"Pink printed Kurta with Trousers with dupatta\\n\\nKurta design:\\nStriped\\nA-line shape\\nEmpire style\\nV-neck, three-quarter regular sleeves\\nThread work detail\\nCalf length with flared hem\\nSilk blend machine weave fabric\\n\\nTrousers design:\\nSolid Trousers\\nPartially elasticated waistband\\nSlip-on closure\\n\\nSize & Fit\\nThe model (height 5'8) is wearing a size S\\n\\nMaterial & Care\\nTop-Silk Blend\\nBottom-Shantoon\\nDupatta-Silk Blend\\nDry Clean\\n\\nSpecifications\\nSleeve Length\\nThree-Quarter Sleeves\\nTop Shape\\nA-Line\\nTop Type\\nKurta\\nBottom Type\\nTrousers\\nDupatta\\nWith Dupatta\\nTop Pattern\\nStriped\\nTop Design Styling\\nEmpire\\nTop Hemline\\nFlared\\nTop Length\\nCalf Length\\nNeck\\nV-Neck\\nPrint or Pattern Type\\nStriped\\nBottom Pattern\\nSolid\\nBottom Closure\\nSlip-On\\nWaistband\\nPartially Elasticated\\nOccasion\\nFestive\\nOrnamentation\\nThread Work\\nWeave Pattern\\nRegular\\nWeave Type\\nMachine Weave\\nPattern Coverage\\nLarge\\nDupatta Pattern\\nDyed\\nDupatta Border\\nPrinted\\nNumber of Items\\n3\",\n",
    "}\n",
    "product_b = {    \"description\": \"\"\"Color may slightly vary due to photography\\nBottomwear fabric: shantoon\\nDupatta length: 2 m\\nDupatta fabric: silk blend\\nPackage contains: 1 kurta, 1 dupatta, 1 pants\\nKurta, Bottomwear & Dupatta\\nDry clean\\nMuslin silk\\nProduct Code: 466764916002\\nMRP: Rs. 7,499.00 inclusive of all taxes(MRP changes as per size selection)\\nNet Qty: 1N\\nMarketed By: FASHOR LIFESTYLE PVT LTD, 28/30,Sudershan pura, Industrial Area, 22 Godam, Jaipur, Rajasthan, 302006\\nManufactured By: FASHOR LIFESTYLE PVT LTD, 28/30,Sudershan pura, Industrial Area, 22 Godam, Jaipur, Rajasthan, 302006\\nCountry Of Origin: India\\nCustomer Care Address: Tower-B, 7th Floor,IBC Knowledge Park,Bannerghatta Main Road,Bhavani Nagar, S.G. Palya,Bengaluru, Karnataka – 560029,Ph: 1800-889-9991,E-mail: customercare@ajio.com\\nCommodity: Women's Kurta Suit Sets\",\n",
    "\"\"\"}\n",
    "# Preprocess and vectorize the data\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([product_a['description'], product_b['description']])\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "# Determine if products are a match based on a threshold\n",
    "threshold = 0.8  # This is an example threshold\n",
    "print(similarity_score)\n",
    "if similarity_score[0][0] > threshold:\n",
    "    print('Products are a match')\n",
    "else:\n",
    "    print('Products are not a match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apps.matcher as matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toolkit_detect_language(file_path: str = None, columns: bool = False)->dict: \n",
    "    if file_path is None or len(file_path) == 0:\n",
    "        raise ValueError(\"'file_path' cannot be empty\")\n",
    "    if type(file_path) != str:\n",
    "        raise TypeError(\"data type compatible only 'str'\")\n",
    "    if type(columns) != bool:\n",
    "        raise TypeError('Columns must be bool')\n",
    "    from transformers import pipeline\n",
    "    pipe = pipeline(\"text-classification\", model=\"ImranzamanML/GEFS-language-detector\")\n",
    "    max_length = 2187\n",
    "    def split_text(text, max_length):\n",
    "        words = text.split()  \n",
    "        words_selected = []\n",
    "        currency_length = 0\n",
    "        for word in words:\n",
    "            if currency_length + len(word) + sum(map(len, words_selected)) <= max_length:\n",
    "                words_selected.append(word)\n",
    "                currency_length += len(word)\n",
    "            else:\n",
    "                break       \n",
    "        return ' '.join(words_selected)\n",
    "    df = matcher.get_df(file_path)\n",
    "    if not columns:         \n",
    "        text_combined = ' '.join(df.apply(lambda row: ', '.join(row.astype(str).unique()), axis=1))\n",
    "        texto_cortado = split_text(text_combined, max_length)\n",
    "        return pipe(texto_cortado)[0]['label']  \n",
    "    # Convert each column to a text series and get unique values\n",
    "    unique_values_by_column = df.apply(lambda col: col.astype(str).unique())\n",
    "    # Iterate over the keys of the dictionary of unique values by column\n",
    "    for column_name, values in unique_values_by_column.items():\n",
    "        # Join the unique values into a single string\n",
    "        complete_text = ' '.join(values)       \n",
    "        # Split the text into segments\n",
    "        divided_text = split_text(complete_text, max_length) \n",
    "        unique_values_by_column[column_name] = pipe(divided_text)[0]['label']\n",
    "    return unique_values_by_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The detected language in the CSV is: es'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher.detect_language_file('files/farmatodo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ivan Rivas\\Documents\\project\\matchX2\\.venv\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from apps.ToolsKit import toolkit_detect_language\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matcherito(file_path_x: str = None, file_path_y: str = None, typematch: str = None, queries: list = []) -> pd.DataFrame:\n",
    "    \n",
    "    \n",
    "    extensions = ['csv', 'json']\n",
    "\n",
    "    if not any(file_path_x.endswith(extension) for extension in extensions) or not any(file_path_y.endswith(extension) for extension in extensions):\n",
    "        raise ValueError(\"The file does not have a valid extension.\")\n",
    "\n",
    "    if file_path_x is None:\n",
    "        raise ValueError(\"required field 'file_path_x'\")\n",
    "    elif file_path_y is None:\n",
    "        raise ValueError(\"required field 'file_path_y'\")\n",
    "    else:\n",
    "        if file_path_x == file_path_y:\n",
    "            raise ValueError(\"Cannot put the same file on both paths\")\n",
    "        if not os.path.exists(file_path_x) or not os.path.exists(file_path_y):\n",
    "            raise FileNotFoundError(\"required file does not exist in the path\")\n",
    "\n",
    "    if typematch is None:\n",
    "        raise ValueError(\"required field 'typematch'\")\n",
    "    elif not typematch in ['exact', 'similar']:#cambiar a variable de entorno con la lista de tipo de match\n",
    "        raise ValueError(\"match type not valid\")\n",
    "    \n",
    "    # if not language in ['es', 'en', 'fr']:\n",
    "        # raise ValueError(\"unsupported language\")\n",
    "\n",
    "    if type(file_path_x) != str or type(file_path_y) != str or type(typematch) != str or type(queries) != list:\n",
    "        raise TypeError(\"Data type error in one of the attributes\")\n",
    "    \n",
    "    if toolkit_detect_language(file_path_x) != toolkit_detect_language(file_path_y):\n",
    "        raise TypeError(\"The language must be the same\")\n",
    "\n",
    "\n",
    "    #start logic for match\n",
    "    if len(queries) == 0:\n",
    "        # if not language['all'] is  None:\n",
    "            # print('hola')\n",
    "            # pass #detect language of file\n",
    "        \n",
    "        \n",
    "\n",
    "        pass #run script auto\n",
    "    else:\n",
    "        # run script manual\n",
    "\n",
    "        # Perform data matching based on the query\n",
    "        if np.ndim(queries) == 1:\n",
    "            # Harmonize indices and get matched data\n",
    "            data_match = harmonize_index_match(df_origin, df_alternative, query)\n",
    "            print(data_match)\n",
    "            # Create a DataFrame for the matched data\n",
    "            df_alternative_result = pd.DataFrame(columns=df_alternative.columns)\n",
    "            df_origin_result = pd.DataFrame(columns=df_origin.columns)\n",
    "\n",
    "            for row in data_match:\n",
    "                # print(df_alternative.iloc[row[1]])\n",
    "                # df_origin_result.loc[len(df_origin_result.index)] = df_origin.iloc[row[0]]\n",
    "\n",
    "                df_alternative_result.loc[len(df_alternative_result.index)] = df_alternative.iloc[row[1]]\n",
    "\n",
    "                # df_alternative_result.loc[len(df_alternative_result.index)] = df_alternative.iloc[row[1]]\n",
    "            # Save the result to JSON with or without duplicates\n",
    "                \n",
    "            # df_alternative_result = df_alternative_result.merge(df_origin_result,how='cross')\n",
    "            if with_out_duplicates == 0:\n",
    "                df_alternative_result.to_json(f'{pathFiles}/result_withDuplicates_simple.json')\n",
    "                # df_origin_result.to_json(f'{pathFiles}/Oresult_withDuplicates_simple.json')\n",
    "            else:\n",
    "                df_alternative_result.drop_duplicates().to_json(f'{pathFiles}/result_withOutDuplicates_simple.json')\n",
    "                # df_origin_result.drop_duplicates().to_json(f'{pathFiles}/Oresult_withOutDuplicates_simple.json')\n",
    "\n",
    "        else:\n",
    "             # Multiple queries case\n",
    "        \n",
    "            if len(query) >= 2:\n",
    "                # Harmonize indices for each query and store the results in a matrix\n",
    "                matriz_data_match = []\n",
    "                for query_x in query:\n",
    "                    matriz_data_match.append(harmonize_index_match(df_origin, df_alternative, query_x, precision=70))\n",
    "                print(matriz_data_match)\n",
    "                # Convert the matrix to a dictionary\n",
    "                variables = arr_to_dic(matriz_data_match)\n",
    "                \n",
    "                # Perform matching on pairs of columns\n",
    "                result = matcher_pair_columns(variables)\n",
    "                \n",
    "                # Save the result to JSON with or without duplicates\n",
    "                if with_out_duplicates == 0:\n",
    "                    df_result = df_alternative.iloc[result].reset_index(drop=True)\n",
    "                    # print(df_result)\n",
    "                    df_result.to_json(f'{pathFiles}/result_withDuplicates_multi.json')\n",
    "                else:\n",
    "                    df_alternative.iloc[result].drop_duplicates().to_json(f'{pathFiles}/result_withOutDuplicates_multi.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matcherito('colores.csv','respaldo.csv', 'exact')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
